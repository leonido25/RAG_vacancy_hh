{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6426f596-1b67-4ea0-acfd-628eba14710a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, MarkdownTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb8c45d9-6032-40ca-a3ec-8bed6f92a4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b1414c7-3756-40f4-80a7-bf40194b6229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from time import sleep\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c19936a5-eb05-4709-ab5f-757ba52e61cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://api.hh.ru/vacancies\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc442c1e-8ba8-4374-9d89-d8f45c55edc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles_list = [\"data scientist\", \"data analyst\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51da221c-f5f7-4261-986c-d13289715a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_params_template = {\n",
    "    'area': 113,           # Регион: Россия\n",
    "    'per_page': 100,       \n",
    "    'order_by': 'publication_time' \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f444b93f-984b-4c07-ad6d-c7b17d37d87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vacancies_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "045dc724-89d9-4239-a116-64bc28346c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PAGES = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "647b43b3-1d15-49af-a660-8c0aa4f6a91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_vacancy(base_params, page_num):\n",
    "    \"\"\"\n",
    "    Выполняет HTTP-запрос к API HH.ru с обновлением номера страницы.\n",
    "    \"\"\"\n",
    "    # 1. КОПИРУЕМ базовый шаблон и обновляем номер страницы (КЛЮЧЕВОЙ ШАГ!)\n",
    "    current_params = base_params.copy() \n",
    "    current_params[\"page\"] = page_num \n",
    "    \n",
    "    try:\n",
    "        response = requests.get(URL, params=current_params)\n",
    "        response.raise_for_status() # Вызывает исключение для 4xx/5xx ошибок\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Ошибка HTTP на странице {page_num}: {e}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Ошибка декодирования JSON на странице {page_num}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c7b36f5-b0e8-4949-901a-5f9bfa80636a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_details(vacancy_id):\n",
    "    \"\"\"\n",
    "    Получает полное описание вакансии по ее ID.\n",
    "    \"\"\"\n",
    "    detail_url = f\"https://api.hh.ru/vacancies/{vacancy_id}\"\n",
    "    try:\n",
    "        response = requests.get(detail_url)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Ошибка получения деталей для ID {vacancy_id}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6ccf2eb4-8f16-4e3e-8307-d3eb1f50a040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. ФУНКЦИЯ ОБРАБОТКИ ДАННЫХ (Сплющивание)\n",
    "def process_vacancy(vacancy, search_term):\n",
    "    salary_data = vacancy.get('salary')\n",
    "    min_salary = None\n",
    "    if salary_data and isinstance(salary_data, dict) and salary_data.get('from'):\n",
    "        min_salary = salary_data['from']\n",
    "        \n",
    "    # 2. Работодатель: вложенное поле\n",
    "    employer_info = vacancy.get('employer', {})\n",
    "    employer_name = employer_info.get('name', 'N/A')\n",
    "    \n",
    "    # 3. Город: вложенное поле\n",
    "    area_info = vacancy.get('area', {})\n",
    "    city_name = area_info.get('name', 'N/A')\n",
    "    \n",
    "    # 4. Требования и обязанности (извлекаем чистый текст)\n",
    "    snippet = vacancy.get('snippet', {})\n",
    "    requirement = snippet.get('requirement', '')\n",
    "    responsibility = snippet.get('responsibility', '')\n",
    "    \n",
    "    # 5. Опыт: вложенное поле\n",
    "    experience_info = vacancy.get('experience', {})\n",
    "    experience_name = experience_info.get('name', 'N/A')\n",
    "\n",
    "    # 1. Получаем ID вакансии\n",
    "    vacancy_id = vacancy.get('id')\n",
    "    full_description = \"N/A\"\n",
    "    \n",
    "    if vacancy_id:\n",
    "        # 2. Делаем новый запрос за деталями\n",
    "        full_data = get_full_details(vacancy_id)\n",
    "        \n",
    "        # 3. Извлекаем поле 'description' (там HTML-код)\n",
    "        if full_data and full_data.get('description'):\n",
    "            full_description = full_data['description']\n",
    "        \n",
    "        # ОЧЕНЬ ВАЖНО: Добавляем небольшую паузу после каждого запроса деталей\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    # Создание плоского словаря\n",
    "    flat_row = {\n",
    "        'full_description': full_description,\n",
    "        'search_term': search_term, # Какую должность мы искали\n",
    "        'vacancy_id': vacancy.get('id'),\n",
    "        'vacancy_name': vacancy.get('name'),\n",
    "        'city_name': city_name,\n",
    "        'min_salary': min_salary,\n",
    "        'employer_name': employer_name,\n",
    "        'published_at': vacancy.get('published_at'),\n",
    "        'experience': experience_name,\n",
    "        'schedule': vacancy.get('schedule', {}).get('name'),\n",
    "        'employment': vacancy.get('employment', {}).get('name'),\n",
    "        'requirement': requirement,\n",
    "        'responsibility': responsibility,\n",
    "        # Добавьте другие поля по необходимости\n",
    "    }\n",
    "    return flat_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ad48892-d2e7-439c-9bbc-e7ff1f713327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Начинаем парсинг для: 'data scientist' ---\n",
      "Обработано 100 вакансий на странице 0. Всего: 100\n",
      "Обработано 100 вакансий на странице 1. Всего: 200\n",
      "Обработано 100 вакансий на странице 2. Всего: 300\n",
      "Обработано 20 вакансий на странице 3. Всего: 320\n",
      "--- Начинаем парсинг для: 'data analyst' ---\n",
      "Обработано 100 вакансий на странице 0. Всего: 420\n",
      "Обработано 100 вакансий на странице 1. Всего: 520\n",
      "Обработано 100 вакансий на странице 2. Всего: 620\n",
      "Обработано 100 вакансий на странице 3. Всего: 720\n"
     ]
    }
   ],
   "source": [
    "# 4. ОСНОВНОЙ ЦИКЛ ПАРСИНГА\n",
    "for job in job_titles_list:\n",
    "    print(f\"--- Начинаем парсинг для: '{job}' ---\")\n",
    "    \n",
    "    # 1. Определяем параметры для текущей профессии (добавляем 'text')\n",
    "    current_search_params = base_params_template.copy()\n",
    "    current_search_params['text'] = job\n",
    "    \n",
    "    for page in range(MAX_PAGES):\n",
    "        vac_data = download_vacancy(current_search_params, page)\n",
    "        \n",
    "        # Если загрузка не удалась (вернула None или пустой словарь)\n",
    "        if not vac_data:\n",
    "            print(f\"Пропуск страницы {page} из-за ошибки запроса.\")\n",
    "            continue # Переходим к следующей итерации цикла\n",
    "        \n",
    "        # БЕЗОПАСНАЯ ПРОВЕРКА (устраняет KeyError, как мы обсуждали)\n",
    "        if 'items' in vac_data:\n",
    "            vacancies_on_page = vac_data['items']\n",
    "            \n",
    "            if not vacancies_on_page:\n",
    "                print(f\"Страница {page} пуста. Завершаем сбор для '{job}'.\")\n",
    "                break # Выходим из цикла по страницам, так как вакансии закончились\n",
    "            \n",
    "            for vacancy in vacancies_on_page:\n",
    "                # 2. Сплющиваем и добавляем плоский словарь в финальный список\n",
    "                flat_row = process_vacancy(vacancy, job)\n",
    "                all_vacancies_data.append(flat_row)\n",
    "                \n",
    "            print(f\"Обработано {len(vacancies_on_page)} вакансий на странице {page}. Всего: {len(all_vacancies_data)}\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"Ошибка структуры данных на странице {page}: Нет ключа 'items'.\")\n",
    "            \n",
    "        time.sleep(0.2) # Пауза между запросами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c61e4014-70fc-49d8-8ada-1ebfb7167aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Завершение сбора данных ---\n",
      "Общее количество собранных записей: 720\n",
      "\n",
      "✅ Датасет успешно создан и сохранен в файл: hh_vacancies_data.csv\n"
     ]
    }
   ],
   "source": [
    "# 5. СОЗДАНИЕ DATAFRAME И CSV\n",
    "print(\"\\n--- Завершение сбора данных ---\")\n",
    "print(f\"Общее количество собранных записей: {len(all_vacancies_data)}\")\n",
    "\n",
    "if all_vacancies_data:\n",
    "    df = pd.DataFrame(all_vacancies_data)\n",
    "    \n",
    "    # Заполняем пустые значения (None) в зарплате нулем для удобства анализа\n",
    "    df['min_salary'] = df['min_salary'].fillna(0) \n",
    "    \n",
    "    filename = 'hh_vacancies_data.csv'\n",
    "    # index=False исключает столбец с индексами из CSV\n",
    "    df.to_csv(filename, index=False, encoding='utf-8')\n",
    "    print(f\"\\n✅ Датасет успешно создан и сохранен в файл: {filename}\")\n",
    "else:\n",
    "    print(\"❌ Список вакансий пуст. Файл CSV не создан.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce022567-2cf2-40d1-bbd4-e13a4ad5c0ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_description</th>\n",
       "      <th>search_term</th>\n",
       "      <th>vacancy_id</th>\n",
       "      <th>vacancy_name</th>\n",
       "      <th>city_name</th>\n",
       "      <th>min_salary</th>\n",
       "      <th>employer_name</th>\n",
       "      <th>published_at</th>\n",
       "      <th>experience</th>\n",
       "      <th>schedule</th>\n",
       "      <th>employment</th>\n",
       "      <th>requirement</th>\n",
       "      <th>responsibility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;p&gt;Требуется разработать агента, который будет...</td>\n",
       "      <td>data scientist</td>\n",
       "      <td>127489893</td>\n",
       "      <td>Data Scientist (Управление моделирования и исс...</td>\n",
       "      <td>Москва</td>\n",
       "      <td>0.0</td>\n",
       "      <td>СБЕР</td>\n",
       "      <td>2025-11-10T17:08:11+0300</td>\n",
       "      <td>От 3 до 6 лет</td>\n",
       "      <td>Полный день</td>\n",
       "      <td>Полная занятость</td>\n",
       "      <td>Опыт работы &lt;highlighttext&gt;Data&lt;/highlighttext...</td>\n",
       "      <td>Сбор требований по моделям и коммуникация с за...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;b&gt;Обязанности:&lt;/b&gt;&lt;br /&gt;-Разрабатывать, внедр...</td>\n",
       "      <td>data scientist</td>\n",
       "      <td>126119482</td>\n",
       "      <td>Data Scientist Middle/Middle+</td>\n",
       "      <td>Москва</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Альфа-Банк</td>\n",
       "      <td>2025-11-10T17:07:29+0300</td>\n",
       "      <td>От 1 года до 3 лет</td>\n",
       "      <td>Полный день</td>\n",
       "      <td>Полная занятость</td>\n",
       "      <td>Уверенное владение Python для промышленной раз...</td>\n",
       "      <td>Разрабатывать, внедрять и оптимизировать класс...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;p&gt;Если ТЕБЕ нравятся крутые и амбициозные зад...</td>\n",
       "      <td>data scientist</td>\n",
       "      <td>127117611</td>\n",
       "      <td>Аналитик данных</td>\n",
       "      <td>Нижний Новгород</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>КСК-Эйч Ар</td>\n",
       "      <td>2025-11-10T16:55:15+0300</td>\n",
       "      <td>Нет опыта</td>\n",
       "      <td>Удаленная работа</td>\n",
       "      <td>Полная занятость</td>\n",
       "      <td>Расширение технических знаний и развитие из ан...</td>\n",
       "      <td>Автоматизация выгрузки и сбора данных. - Ad/ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;p&gt;Если ТЕБЕ нравятся крутые и амбициозные зад...</td>\n",
       "      <td>data scientist</td>\n",
       "      <td>127117612</td>\n",
       "      <td>Аналитик данных</td>\n",
       "      <td>Самара</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>КСК-Эйч Ар</td>\n",
       "      <td>2025-11-10T16:55:15+0300</td>\n",
       "      <td>Нет опыта</td>\n",
       "      <td>Удаленная работа</td>\n",
       "      <td>Полная занятость</td>\n",
       "      <td>Расширение технических знаний и развитие из ан...</td>\n",
       "      <td>Автоматизация выгрузки и сбора данных. - Ad/ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;p&gt;​​​Наша команда каждый день работает над по...</td>\n",
       "      <td>data scientist</td>\n",
       "      <td>126737674</td>\n",
       "      <td>Старший бизнес-аналитик CLTV</td>\n",
       "      <td>Москва</td>\n",
       "      <td>0.0</td>\n",
       "      <td>билайн</td>\n",
       "      <td>2025-11-10T15:20:32+0300</td>\n",
       "      <td>От 3 до 6 лет</td>\n",
       "      <td>Удаленная работа</td>\n",
       "      <td>Полная занятость</td>\n",
       "      <td>Высшее образование в сфере экономики, математи...</td>\n",
       "      <td>...пилоты и А/В тесты. Оформлять инсайты в пре...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    full_description     search_term  \\\n",
       "0  <p>Требуется разработать агента, который будет...  data scientist   \n",
       "1  <b>Обязанности:</b><br />-Разрабатывать, внедр...  data scientist   \n",
       "2  <p>Если ТЕБЕ нравятся крутые и амбициозные зад...  data scientist   \n",
       "3  <p>Если ТЕБЕ нравятся крутые и амбициозные зад...  data scientist   \n",
       "4  <p>​​​Наша команда каждый день работает над по...  data scientist   \n",
       "\n",
       "  vacancy_id                                       vacancy_name  \\\n",
       "0  127489893  Data Scientist (Управление моделирования и исс...   \n",
       "1  126119482                      Data Scientist Middle/Middle+   \n",
       "2  127117611                                    Аналитик данных   \n",
       "3  127117612                                    Аналитик данных   \n",
       "4  126737674                       Старший бизнес-аналитик CLTV   \n",
       "\n",
       "         city_name  min_salary employer_name              published_at  \\\n",
       "0           Москва         0.0          СБЕР  2025-11-10T17:08:11+0300   \n",
       "1           Москва         0.0    Альфа-Банк  2025-11-10T17:07:29+0300   \n",
       "2  Нижний Новгород     40000.0    КСК-Эйч Ар  2025-11-10T16:55:15+0300   \n",
       "3           Самара     40000.0    КСК-Эйч Ар  2025-11-10T16:55:15+0300   \n",
       "4           Москва         0.0        билайн  2025-11-10T15:20:32+0300   \n",
       "\n",
       "           experience          schedule        employment  \\\n",
       "0       От 3 до 6 лет       Полный день  Полная занятость   \n",
       "1  От 1 года до 3 лет       Полный день  Полная занятость   \n",
       "2           Нет опыта  Удаленная работа  Полная занятость   \n",
       "3           Нет опыта  Удаленная работа  Полная занятость   \n",
       "4       От 3 до 6 лет  Удаленная работа  Полная занятость   \n",
       "\n",
       "                                         requirement  \\\n",
       "0  Опыт работы <highlighttext>Data</highlighttext...   \n",
       "1  Уверенное владение Python для промышленной раз...   \n",
       "2  Расширение технических знаний и развитие из ан...   \n",
       "3  Расширение технических знаний и развитие из ан...   \n",
       "4  Высшее образование в сфере экономики, математи...   \n",
       "\n",
       "                                      responsibility  \n",
       "0  Сбор требований по моделям и коммуникация с за...  \n",
       "1  Разрабатывать, внедрять и оптимизировать класс...  \n",
       "2  Автоматизация выгрузки и сбора данных. - Ad/ho...  \n",
       "3  Автоматизация выгрузки и сбора данных. - Ad/ho...  \n",
       "4  ...пилоты и А/В тесты. Оформлять инсайты в пре...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "74b947b4-e15c-4172-991b-746ea9204b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "employer_name\n",
       "СБЕР                50\n",
       "Ozon                22\n",
       "билайн              16\n",
       "Альфа-Банк          13\n",
       "X5 Tech             11\n",
       "                    ..\n",
       "Evercode Lab         1\n",
       "Cloud.ru             1\n",
       "АльфаСтрахование     1\n",
       "Мэлон Фэшн Груп      1\n",
       "Инженеры продаж      1\n",
       "Name: count, Length: 416, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['employer_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5683beca-4a9c-40fd-b0de-f4dbf60838db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<p>Требуется разработать агента, который будет оркестрировать несколько ML для анализа биржевых стаканов облигаций и на основании этого предлагает оптимальные варианты для совершения сделок покупки/продажи бумаг трейдерам.</p> <h3><strong>Обязанности</strong></h3> <ul> <li>сбор требований по моделям и коммуникация с заказчиком и DE</li> <li>определение целевых метрик решения и согласование их с заказчиком</li> <li>сбор и подготовка данных (Hadoop, pyspark, написание парсеров для сборки внешних данных)</li> <li>управление ресурсом Middle DS</li> <li>построение моделей по табличным данным</li> <li>поиск аномалий во временных рядах</li> <li>построение моделей предсказания временных рядов</li> <li>классификация новостей по степени влияния на рынок облигаций</li> <li>генерации торговых сигналов на основе неструктурированных данных</li> <li>создание RAG базы для генерации текстовых обоснований рекомендаций для трейдеров</li> <li>адаптация LLM для формулировки торговых идей на естественном языке</li> <li>тестирование различных гипотез.</li> </ul> <h3><strong>Требования</strong></h3> <ul> <li>опыт работы Data Scientist от 2 лет</li> <li>образование по направлениям &quot;Математика&quot;, «Физика», «Мат. методы в экономике», (желательно выпускники - МГУ, МФТИ, ВШЭ и других ведущих вузов страны)</li> <li>глубокие знания в области теории вероятностей и математической статистики</li> <li>навыки написания качественного кода на Python (знание ООП, умение написания оптимального кода по скорости и памяти)</li> <li>понимания работы алгоритмов ML-моделей (бустинг, нейронные сети, NLP-модели, LLM)</li> <li>владения Python несколькими библиотеками для классического ML из списка или релевантными (numpy, sklearn, pandas, scikit-learn, matplotlib/seaborn/plotly, catboost/lightgbm/xgboost, fbprophet, pygam)</li> <li>владения pytorch или другими DL фреймворками</li> <li>практика с нейросетевыми моделями (RNN, LSTM, Transformer, BERT)</li> <li>опыт разработки NLP-моделей (NLTK, spaCy, gensim, transformers).</li> </ul> <p><strong>Будет плюсом:</strong></p> <ul> <li>понимание работы рынков ценных бумаг или готовность развиваться в этой области:</li> <li>знание фин. метрик: YTM, duration и т.д.</li> <li>опыт работы с кривыми доходностей и понимание оценки риска ценных бумаг</li> <li>понимание концепции парной торговли</li> <li>практические навыки fine-tuning и инференса LLM</li> <li>опыт разработки и оптимизация RAG-пайплайнов.</li> </ul> <h3><strong>Условия</strong></h3> <ul> <li>комфортный современный офис рядом с метро Кутузовская</li> <li>возможность выбрать удобный график – офис/гибрид</li> <li>ежегодный пересмотр зарплаты и годовая премия</li> <li>корпоративный спортзал и зоны отдыха</li> <li>более 400 образовательных программ СберУниверситета для профессионального и карьерного развития</li> <li>расширенный ДМС, льготное страхование для семьи и корпоративная пенсионная программа</li> <li>гибкий дисконт по ипотечному кредиту, равный 1/3 ключевой ставки ЦБ</li> <li>бесплатная подписка СберПрайм+, скидки на продукты компаний-партнеров.</li> </ul>'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['full_description'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6faf2f6-4232-4287-b99d-f836ec32c835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "670"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#число уникальных описаний\n",
    "unique_description = df['full_description'].nunique()\n",
    "unique_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a804c93f-f704-45ea-b855-78618f2ab470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Общее количество вакансий: 720\n",
      "Количество уникальных работодателей: 416\n"
     ]
    }
   ],
   "source": [
    "unique_employers = df['employer_name'].nunique()\n",
    "total_vacancies = len(df)\n",
    "print(f\"Общее количество вакансий: {total_vacancies}\")\n",
    "print(f\"Количество уникальных работодателей: {unique_employers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6f169d31-0dde-4a3e-8be1-fc76aaa3ecfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Топ-10 работодателей:\n",
      "employer_name\n",
      "СБЕР           50\n",
      "Ozon           22\n",
      "билайн         16\n",
      "Альфа-Банк     13\n",
      "X5 Tech        11\n",
      "Т-Банк         11\n",
      "МТС            11\n",
      "WILDBERRIES     9\n",
      "Aston           7\n",
      "LIAN            6\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "top_employers = df['employer_name'].value_counts().head(10)\n",
    "print(\"\\nТоп-10 работодателей:\")\n",
    "print(top_employers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dd2ad025-f720-4a30-b5d5-8c02f5ee2b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_alt = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1b9e28-1eb9-44ba-90d1-856574b5cc62",
   "metadata": {},
   "source": [
    "создам порог и почищу датасет от нулевых значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a41f9d62-2cff-49eb-9d60-71d4b0c8dc7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "full_description    720\n",
       "search_term         720\n",
       "vacancy_id          720\n",
       "vacancy_name        720\n",
       "city_name           720\n",
       "min_salary          720\n",
       "employer_name       720\n",
       "published_at        720\n",
       "experience          720\n",
       "schedule            720\n",
       "employment          720\n",
       "requirement         715\n",
       "responsibility      718\n",
       "dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_alt.notnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c2a3e709-de2e-4ea4-9f35-1c5718e8a0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_alt = df_alt.loc[:, df_alt.notnull().sum() >= treshhold]\n",
    "#df_alt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ccc67006-e661-4b78-bc0f-dca115247379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_description</th>\n",
       "      <th>search_term</th>\n",
       "      <th>vacancy_id</th>\n",
       "      <th>vacancy_name</th>\n",
       "      <th>city_name</th>\n",
       "      <th>min_salary</th>\n",
       "      <th>employer_name</th>\n",
       "      <th>published_at</th>\n",
       "      <th>experience</th>\n",
       "      <th>schedule</th>\n",
       "      <th>employment</th>\n",
       "      <th>requirement</th>\n",
       "      <th>responsibility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;p&gt;Требуется разработать агента, который будет...</td>\n",
       "      <td>data scientist</td>\n",
       "      <td>127489893</td>\n",
       "      <td>Data Scientist (Управление моделирования и исс...</td>\n",
       "      <td>Москва</td>\n",
       "      <td>0.0</td>\n",
       "      <td>СБЕР</td>\n",
       "      <td>2025-11-10T17:08:11+0300</td>\n",
       "      <td>От 3 до 6 лет</td>\n",
       "      <td>Полный день</td>\n",
       "      <td>Полная занятость</td>\n",
       "      <td>Опыт работы &lt;highlighttext&gt;Data&lt;/highlighttext...</td>\n",
       "      <td>Сбор требований по моделям и коммуникация с за...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;b&gt;Обязанности:&lt;/b&gt;&lt;br /&gt;-Разрабатывать, внедр...</td>\n",
       "      <td>data scientist</td>\n",
       "      <td>126119482</td>\n",
       "      <td>Data Scientist Middle/Middle+</td>\n",
       "      <td>Москва</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Альфа-Банк</td>\n",
       "      <td>2025-11-10T17:07:29+0300</td>\n",
       "      <td>От 1 года до 3 лет</td>\n",
       "      <td>Полный день</td>\n",
       "      <td>Полная занятость</td>\n",
       "      <td>Уверенное владение Python для промышленной раз...</td>\n",
       "      <td>Разрабатывать, внедрять и оптимизировать класс...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    full_description     search_term  \\\n",
       "0  <p>Требуется разработать агента, который будет...  data scientist   \n",
       "1  <b>Обязанности:</b><br />-Разрабатывать, внедр...  data scientist   \n",
       "\n",
       "  vacancy_id                                       vacancy_name city_name  \\\n",
       "0  127489893  Data Scientist (Управление моделирования и исс...    Москва   \n",
       "1  126119482                      Data Scientist Middle/Middle+    Москва   \n",
       "\n",
       "   min_salary employer_name              published_at          experience  \\\n",
       "0         0.0          СБЕР  2025-11-10T17:08:11+0300       От 3 до 6 лет   \n",
       "1         0.0    Альфа-Банк  2025-11-10T17:07:29+0300  От 1 года до 3 лет   \n",
       "\n",
       "      schedule        employment  \\\n",
       "0  Полный день  Полная занятость   \n",
       "1  Полный день  Полная занятость   \n",
       "\n",
       "                                         requirement  \\\n",
       "0  Опыт работы <highlighttext>Data</highlighttext...   \n",
       "1  Уверенное владение Python для промышленной раз...   \n",
       "\n",
       "                                      responsibility  \n",
       "0  Сбор требований по моделям и коммуникация с за...  \n",
       "1  Разрабатывать, внедрять и оптимизировать класс...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_alt.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcef733-8c49-4894-8ce3-bc3e75e529d5",
   "metadata": {},
   "source": [
    "Очищу датасет до минимума необходимой информации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bbbd9b31-f20f-4143-bce2-60c0bc391992",
   "metadata": {},
   "outputs": [],
   "source": [
    "#features_to_save = ['id', 'name', 'full_description', 'snippet.responsibility', 'snippet.requirement',\n",
    "                    #'schedule.name', 'experience.name', 'employment.name', 'address.city']\n",
    "#df_final = df_alt[features_to_save]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8754fba2-0603-4388-8a1b-fa570b225c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_final.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7e308929-2dc8-47eb-b329-c04d1395f744",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_alt.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30ba23e-f72e-4cbd-b982-eabbfb649f9e",
   "metadata": {},
   "source": [
    "очищу данные от html символов, пунктуации и тд"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7ba4ad4f-134f-4610-b882-34ebf5471555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import unicodedata\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "59382526-dcf1-4724-af52-71e7975cd261",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ad192c10-6fd1-4063-a858-2a44aeff9c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 12:08:12 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 435kB [00:00, 3.78MB/s]                    \n",
      "2025-11-11 12:08:13 INFO: Downloaded file to C:\\Users\\User\\stanza_resources\\resources.json\n",
      "2025-11-11 12:08:13 INFO: Loading these models for language: ru (Russian):\n",
      "==================================\n",
      "| Processor | Package            |\n",
      "----------------------------------\n",
      "| tokenize  | syntagrus          |\n",
      "| lemma     | syntagrus_nocharlm |\n",
      "==================================\n",
      "\n",
      "2025-11-11 12:08:13 INFO: Using device: cpu\n",
      "2025-11-11 12:08:13 INFO: Loading: tokenize\n",
      "2025-11-11 12:08:13 INFO: Loading: lemma\n",
      "2025-11-11 12:08:16 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp_ru = stanza.Pipeline(lang='ru', processors='tokenize,lemma')\n",
    "russian_stopwords = set(stopwords.words('russian'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7dbc4b21-2778-4ea8-be3b-5d42e3afbd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_simple(input_text, stop_words_set=None):\n",
    "    \"\"\"\n",
    "    Упрощенная улучшенная версия без лемматизации\n",
    "    \"\"\"\n",
    "    if pd.isna(input_text) or input_text is None:\n",
    "        return \"\"\n",
    "\n",
    "    text = str(input_text)\n",
    "    \n",
    "    # 1. Удаляем HTML\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    \n",
    "    # 2. Удаляем URL\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
    "    \n",
    "    # 3. Сохраняем базовую структуру текста\n",
    "    text = re.sub(r'[^a-zA-Zа-яА-ЯёЁ0-9\\s\\-_\\.]', ' ', text)\n",
    "    \n",
    "    # 4. Нижний регистр\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 5. Обрабатываем составные слова\n",
    "    text = re.sub(r'(\\b\\w+)-(\\w+\\b)', r'\\1_\\2', text)\n",
    "    \n",
    "    # 6. Токенизация\n",
    "    words = text.split()\n",
    "    \n",
    "    # 7. Фильтрация\n",
    "    if stop_words_set:\n",
    "        filtered_words = [\n",
    "            word for word in words \n",
    "            if (word not in stop_words_set and \n",
    "                len(word) > 1 and\n",
    "                not word.isdigit())  # убираем чистые числа\n",
    "        ]\n",
    "    else:\n",
    "        filtered_words = [word for word in words if len(word) > 1 and not word.isdigit()]\n",
    "    \n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a6e20e3d-9408-4aa5-9597-d555a816f62d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def clean_and_lemmatize_russian(input_text, nlp_pipeline, stop_words_set):    \\n    if pd.isna(input_text) or input_text is None:\\n        return \"\"\\n\\n    clean_text = str(input_text)\\n    # HTML-теги\\n    clean_text = re.sub(\\'<[^<]+?>\\', \\'\\', clean_text)\\n\\n    # URL и ссылки\\n    clean_text = re.sub(r\\'http\\\\S+\\', \\'\\', clean_text)\\n\\n    # Email\\n    clean_text = re.sub(r\\'\\\\S+@\\\\S+\\', \\'\\', clean_text)\\n\\n    clean_text = clean_text.lower()\\n\\n    # Заменяем нежелательные символы на пробелы, разрешаем буквы, цифры и символы: -_+=/&\\n    clean_text = re.sub(r\\'[^a-zA-Zа-яА-ЯёЁ0-9\\\\-_+=/&]\\', \\' \\', clean_text)\\n\\n    # 5. Разделение на слова и удаление стоп-слов\\n    words = clean_text.split(\\' \\')\\n\\n    # Убираем все пробелы\\n    clean_text = re.sub(\\'\\\\s+\\', \\' \\', clean_text)\\n\\n    # Фильтруем слова, убирая стоп-слова и пустые строки (возникшие при очистке)\\n    filtered_words: List[str] = [\\n        word for word in words \\n        if word and word not in stop_words_set\\n    ]\\n\\n    # Возвращаем очищенный текст, состоящий из оригинальных слов (без лемматизации)\\n    return \\' \\'.join(filtered_words)'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Функция для очистки текста\n",
    "'''def clean_and_lemmatize_russian(input_text, nlp_pipeline, stop_words_set):    \n",
    "    if pd.isna(input_text) or input_text is None:\n",
    "        return \"\"\n",
    "\n",
    "    clean_text = str(input_text)\n",
    "    # HTML-теги\n",
    "    clean_text = re.sub('<[^<]+?>', '', clean_text)\n",
    "    \n",
    "    # URL и ссылки\n",
    "    clean_text = re.sub(r'http\\S+', '', clean_text)\n",
    "\n",
    "    # Email\n",
    "    clean_text = re.sub(r'\\S+@\\S+', '', clean_text)\n",
    "\n",
    "    clean_text = clean_text.lower()\n",
    "\n",
    "    # Заменяем нежелательные символы на пробелы, разрешаем буквы, цифры и символы: -_+=/&\n",
    "    clean_text = re.sub(r'[^a-zA-Zа-яА-ЯёЁ0-9\\-_+=/&]', ' ', clean_text)\n",
    "\n",
    "    # 5. Разделение на слова и удаление стоп-слов\n",
    "    words = clean_text.split(' ')\n",
    "\n",
    "    # Убираем все пробелы\n",
    "    clean_text = re.sub('\\s+', ' ', clean_text)\n",
    "    \n",
    "    # Фильтруем слова, убирая стоп-слова и пустые строки (возникшие при очистке)\n",
    "    filtered_words: List[str] = [\n",
    "        word for word in words \n",
    "        if word and word not in stop_words_set\n",
    "    ]\n",
    "    \n",
    "    # Возвращаем очищенный текст, состоящий из оригинальных слов (без лемматизации)\n",
    "    return ' '.join(filtered_words)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "04282f18-1b27-46be-ab5b-2d81a6d11ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'О компании Мы — успешный fashion-бренд с 15-летней историей и собственным производством женской одежды. Полный цикл: от закупки ткани до продажи на маркетплейсах. Наш основной оборот идёт через Wildberries и Ozon (входим в топ-1% продавцов одежды на Ozon ). Сейчас мы запускаем проект по построению аналитической/рекомендательной платформы с нуля . Цель — сделать управление ассортиментом, ценами, закупками, производством и рекламой максимально прозрачным и основанным на данных . Если внутренняя система покажет результат, мы планируем вывести её на рынок как отдельный SaaS-продукт для других продавцов маркетплейсов. Это возможность присоединиться на старте и влиять на архитектуру продукта.(уже есть заинтересованные селлеры) Задачи 1. Сбор данных и интеграции Подключение API Wildberries, Ozon и других площадок. Получение данных: Продажи (выручка, заказы, выкупы, возвраты, статусы). Остатки на складах, логистика. Рекламные метрики (показы, клики, CTR, CPC, CPA, ROI). Цены, динамика скидок, участие в акциях. Позиции в поиске, отзывы, рейтинг. Интеграция данных из 1С (по ODATA) . Загрузка данных из Excel/CSV. Парсинг маркетплейсов для конкурентного анализа. 2. Построение хранилища данных Проектирование и реализация Data LakeHouse . Хранение данных в S3 или аналогах , первичная обработка. Создание структуры данных (сырые → очищенные → витрины). Использование ClickHouse или других колоночных БД. Настройка базовых ETL/ELT-процессов . 3. Аналитика и прогнозирование Прогноз спроса и продаж (по товарам и категориям). Оптимизация остатков и распределение поставок по складам WB/Ozon . Автоматизация рекламных кампаний: динамические ставки, удаление неэффективных ключей и кластеров, анализ CTR/конверсий. Расчёт маржинальности и прибыли. Оценка эффективности SKU, поставок и логистики. 4. Визуализация и отчётность Построение дашбордов в Yandex DataLens . Создание таблиц и аналитических отчётов. Возможна разработка части аналитики в самописном веб-приложении . Требования Опыт работы с данными: ETL, SQL, Python . Знание API: работа с REST API маркетплейсов, интеграция с 1С ODATA. Опыт с БД: PostgreSQL, ClickHouse (или аналогами). Инструменты: Docker, Git . Библиотеки: pandas, requests, airflow (или опыт других пайплайн-менеджеров). BI: DataLens (обязательно), понимание метрик аналитики. Умение строить прогнозы (time series, ML — плюс). Умение самостоятельно доводить задачи до результата. Плюсом будет Опыт проектирования Data LakeHouse. Опыт работы с маркетплейсами (WB/Ozon). Знание MLOps. Опыт работы в e-commerce или производстве. Навыки FastAPI/Flask для интеграций и сервисов. Мы предлагаем Участие в проекте с нуля — ключевая роль. Возможность влиять на архитектуру и решения. Рост вместе с продуктом. Гибкий график, гибридный формат (после ИС — частично удалёнка). Реальная свобода выбора технологий и решений. Долгосрочная работа над продуктом, а не «таск-менеджмент». Важно — перед откликом Мы ищем самостоятельного специалиста , а не стажёра или начинающего уровня. На старте у нас нет ресурсов учить с нуля , поэтому: ❗ Если вы не умеете или не готовы быстро научиться: работать с API WB/Ozon, ETL, Python, работать с ClickHouse/PostgreSQL, строить дашборды в DataLens(или других системах) — — не откликайтесь на вакансию. В сопроводительном письме укажите: Ваш уровень по навыкам: Python / SQL / API / ClickHouse / DataLens (по 10-балльной шкале). Реальный опыт: 2–3 проекта или задачи, которыми вы гордитесь. Желательный уровень дохода и формат работы (офис/гибрид). — на испытательный срок мы ищем сотрудников исключительно в офис!!! , не откликайтесь на вакансию если это вас не устраивает! p.s.: Мы не хотим делать копию существующих на рынке продуктов, мы тестировали многие, но они не отвечают нашим задачам и задачам селлеров с кем мы знакомы, у нас уже есть некоторые наработки, поэтому это скорее гибрид нескольких продуктов, чтобы максимально автоматизировать рутинные задачи и быстро принимать управленческие решения.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8ded6fdf-8719-4c91-809d-91de750e4261",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = clean_text_simple(text,  russian_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2ec4e9fb-4e1a-4408-ba82-bc53348fa15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "компании успешный fashion_бренд 15_летней историей собственным производством женской одежды. полный цикл закупки ткани продажи маркетплейсах. наш основной оборот идёт wildberries ozon входим топ_1 продавцов одежды ozon запускаем проект построению аналитической рекомендательной платформы нуля цель сделать управление ассортиментом ценами закупками производством рекламой максимально прозрачным основанным данных внутренняя система покажет результат планируем вывести её рынок отдельный saas_продукт других продавцов маркетплейсов. это возможность присоединиться старте влиять архитектуру продукта. заинтересованные селлеры задачи 1. сбор данных интеграции подключение api wildberries ozon других площадок. получение данных продажи выручка заказы выкупы возвраты статусы остатки складах логистика. рекламные метрики показы клики ctr cpc cpa roi цены динамика скидок участие акциях. позиции поиске отзывы рейтинг. интеграция данных 1с odata загрузка данных excel csv. парсинг маркетплейсов конкурентного анализа. 2. построение хранилища данных проектирование реализация data lakehouse хранение данных s3 аналогах первичная обработка. создание структуры данных сырые очищенные витрины использование clickhouse других колоночных бд. настройка базовых etl elt_процессов 3. аналитика прогнозирование прогноз спроса продаж товарам категориям оптимизация остатков распределение поставок складам wb ozon автоматизация рекламных кампаний динамические ставки удаление неэффективных ключей кластеров анализ ctr конверсий. расчёт маржинальности прибыли. оценка эффективности sku поставок логистики. 4. визуализация отчётность построение дашбордов yandex datalens создание таблиц аналитических отчётов. возможна разработка части аналитики самописном веб_приложении требования опыт работы данными etl sql python знание api работа rest api маркетплейсов интеграция 1с odata. опыт бд postgresql clickhouse аналогами инструменты docker git библиотеки pandas requests airflow опыт других пайплайн_менеджеров bi datalens обязательно понимание метрик аналитики. умение строить прогнозы time series ml плюс умение самостоятельно доводить задачи результата. плюсом опыт проектирования data lakehouse. опыт работы маркетплейсами wb ozon знание mlops. опыт работы e_commerce производстве. навыки fastapi flask интеграций сервисов. предлагаем участие проекте нуля ключевая роль. возможность влиять архитектуру решения. рост вместе продуктом. гибкий график гибридный формат ис частично удалёнка реальная свобода выбора технологий решений. долгосрочная работа продуктом таск_менеджмент важно откликом ищем самостоятельного специалиста стажёра начинающего уровня. старте ресурсов учить нуля поэтому умеете готовы быстро научиться работать api wb ozon etl python работать clickhouse postgresql строить дашборды datalens других системах откликайтесь вакансию. сопроводительном письме укажите ваш уровень навыкам python sql api clickhouse datalens 10_балльной шкале реальный опыт проекта задачи которыми гордитесь. желательный уровень дохода формат работы офис гибрид испытательный срок ищем сотрудников исключительно офис откликайтесь вакансию это устраивает p.s. хотим делать копию существующих рынке продуктов тестировали многие отвечают нашим задачам задачам селлеров кем знакомы некоторые наработки поэтому это скорее гибрид нескольких продуктов максимально автоматизировать рутинные задачи быстро принимать управленческие решения.\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28816d79-57f1-417b-a2c0-2c0e395e0131",
   "metadata": {},
   "source": [
    "Обработаю текстовые фичи функцией для удаления html, пунктуации, приведения кстандартному виду"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e03093e1-2f44-4e8a-a0d5-2e5709451977",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_alt['description_cleaned'] = df_alt['full_description'].apply(\n",
    "    lambda text: clean_text_simple(text, russian_stopwords)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "37fa161c-6bcd-4f59-bd39-917d7adc715d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_alt['requirement_cleaned'] = df_alt['requirement'].apply(\n",
    "    lambda text: clean_text_simple(text, russian_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0f1937a1-8057-4e52-9c93-8fb96765086e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'уверенное владение python промышленной разработки. опыт работы pyspark sql обработки больших данных. глубокие знания классических методов машинного...'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_alt['requirement_cleaned'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6f7e41a7-3086-472a-a21c-2a7f1589c251",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_alt.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ab533a2f-fc74-4972-be11-e5fb6e587bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.drop(['full_description', 'requirement'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9c5253f8-a6bd-40d8-824e-c371ae5736de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>search_term</th>\n",
       "      <th>vacancy_id</th>\n",
       "      <th>vacancy_name</th>\n",
       "      <th>city_name</th>\n",
       "      <th>min_salary</th>\n",
       "      <th>employer_name</th>\n",
       "      <th>published_at</th>\n",
       "      <th>experience</th>\n",
       "      <th>schedule</th>\n",
       "      <th>employment</th>\n",
       "      <th>responsibility</th>\n",
       "      <th>description_lemmatized</th>\n",
       "      <th>requirement_lemmatized</th>\n",
       "      <th>description_cleaned</th>\n",
       "      <th>requirement_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data scientist</td>\n",
       "      <td>127489893</td>\n",
       "      <td>Data Scientist (Управление моделирования и исс...</td>\n",
       "      <td>Москва</td>\n",
       "      <td>0.0</td>\n",
       "      <td>СБЕР</td>\n",
       "      <td>2025-11-10T17:08:11+0300</td>\n",
       "      <td>От 3 до 6 лет</td>\n",
       "      <td>Полный день</td>\n",
       "      <td>Полная занятость</td>\n",
       "      <td>Сбор требований по моделям и коммуникация с за...</td>\n",
       "      <td>требуется разработать агента который оркестрир...</td>\n",
       "      <td>опыт работы data scientist лет. образование на...</td>\n",
       "      <td>требуется разработать агента который оркестрир...</td>\n",
       "      <td>опыт работы data scientist лет. образование на...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data scientist</td>\n",
       "      <td>126119482</td>\n",
       "      <td>Data Scientist Middle/Middle+</td>\n",
       "      <td>Москва</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Альфа-Банк</td>\n",
       "      <td>2025-11-10T17:07:29+0300</td>\n",
       "      <td>От 1 года до 3 лет</td>\n",
       "      <td>Полный день</td>\n",
       "      <td>Полная занятость</td>\n",
       "      <td>Разрабатывать, внедрять и оптимизировать класс...</td>\n",
       "      <td>обязанности -разрабатывать внедрять оптимизиро...</td>\n",
       "      <td>уверенное владение python промышленной разрабо...</td>\n",
       "      <td>обязанности -разрабатывать внедрять оптимизиро...</td>\n",
       "      <td>уверенное владение python промышленной разрабо...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data scientist</td>\n",
       "      <td>127117611</td>\n",
       "      <td>Аналитик данных</td>\n",
       "      <td>Нижний Новгород</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>КСК-Эйч Ар</td>\n",
       "      <td>2025-11-10T16:55:15+0300</td>\n",
       "      <td>Нет опыта</td>\n",
       "      <td>Удаленная работа</td>\n",
       "      <td>Полная занятость</td>\n",
       "      <td>Автоматизация выгрузки и сбора данных. - Ad/ho...</td>\n",
       "      <td>тебе нравятся крутые амбициозные задачи мечтае...</td>\n",
       "      <td>расширение технических знаний развитие аналити...</td>\n",
       "      <td>тебе нравятся крутые амбициозные задачи мечтае...</td>\n",
       "      <td>расширение технических знаний развитие аналити...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data scientist</td>\n",
       "      <td>127117612</td>\n",
       "      <td>Аналитик данных</td>\n",
       "      <td>Самара</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>КСК-Эйч Ар</td>\n",
       "      <td>2025-11-10T16:55:15+0300</td>\n",
       "      <td>Нет опыта</td>\n",
       "      <td>Удаленная работа</td>\n",
       "      <td>Полная занятость</td>\n",
       "      <td>Автоматизация выгрузки и сбора данных. - Ad/ho...</td>\n",
       "      <td>тебе нравятся крутые амбициозные задачи мечтае...</td>\n",
       "      <td>расширение технических знаний развитие аналити...</td>\n",
       "      <td>тебе нравятся крутые амбициозные задачи мечтае...</td>\n",
       "      <td>расширение технических знаний развитие аналити...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data scientist</td>\n",
       "      <td>126737674</td>\n",
       "      <td>Старший бизнес-аналитик CLTV</td>\n",
       "      <td>Москва</td>\n",
       "      <td>0.0</td>\n",
       "      <td>билайн</td>\n",
       "      <td>2025-11-10T15:20:32+0300</td>\n",
       "      <td>От 3 до 6 лет</td>\n",
       "      <td>Удаленная работа</td>\n",
       "      <td>Полная занятость</td>\n",
       "      <td>...пилоты и А/В тесты. Оформлять инсайты в пре...</td>\n",
       "      <td>наша команда каждый день работает повышением к...</td>\n",
       "      <td>высшее образование сфере экономики математики ...</td>\n",
       "      <td>наша команда каждый день работает повышением к...</td>\n",
       "      <td>высшее образование сфере экономики математики ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      search_term vacancy_id  \\\n",
       "0  data scientist  127489893   \n",
       "1  data scientist  126119482   \n",
       "2  data scientist  127117611   \n",
       "3  data scientist  127117612   \n",
       "4  data scientist  126737674   \n",
       "\n",
       "                                        vacancy_name        city_name  \\\n",
       "0  Data Scientist (Управление моделирования и исс...           Москва   \n",
       "1                      Data Scientist Middle/Middle+           Москва   \n",
       "2                                    Аналитик данных  Нижний Новгород   \n",
       "3                                    Аналитик данных           Самара   \n",
       "4                       Старший бизнес-аналитик CLTV           Москва   \n",
       "\n",
       "   min_salary employer_name              published_at          experience  \\\n",
       "0         0.0          СБЕР  2025-11-10T17:08:11+0300       От 3 до 6 лет   \n",
       "1         0.0    Альфа-Банк  2025-11-10T17:07:29+0300  От 1 года до 3 лет   \n",
       "2     40000.0    КСК-Эйч Ар  2025-11-10T16:55:15+0300           Нет опыта   \n",
       "3     40000.0    КСК-Эйч Ар  2025-11-10T16:55:15+0300           Нет опыта   \n",
       "4         0.0        билайн  2025-11-10T15:20:32+0300       От 3 до 6 лет   \n",
       "\n",
       "           schedule        employment  \\\n",
       "0       Полный день  Полная занятость   \n",
       "1       Полный день  Полная занятость   \n",
       "2  Удаленная работа  Полная занятость   \n",
       "3  Удаленная работа  Полная занятость   \n",
       "4  Удаленная работа  Полная занятость   \n",
       "\n",
       "                                      responsibility  \\\n",
       "0  Сбор требований по моделям и коммуникация с за...   \n",
       "1  Разрабатывать, внедрять и оптимизировать класс...   \n",
       "2  Автоматизация выгрузки и сбора данных. - Ad/ho...   \n",
       "3  Автоматизация выгрузки и сбора данных. - Ad/ho...   \n",
       "4  ...пилоты и А/В тесты. Оформлять инсайты в пре...   \n",
       "\n",
       "                              description_lemmatized  \\\n",
       "0  требуется разработать агента который оркестрир...   \n",
       "1  обязанности -разрабатывать внедрять оптимизиро...   \n",
       "2  тебе нравятся крутые амбициозные задачи мечтае...   \n",
       "3  тебе нравятся крутые амбициозные задачи мечтае...   \n",
       "4  наша команда каждый день работает повышением к...   \n",
       "\n",
       "                              requirement_lemmatized  \\\n",
       "0  опыт работы data scientist лет. образование на...   \n",
       "1  уверенное владение python промышленной разрабо...   \n",
       "2  расширение технических знаний развитие аналити...   \n",
       "3  расширение технических знаний развитие аналити...   \n",
       "4  высшее образование сфере экономики математики ...   \n",
       "\n",
       "                                 description_cleaned  \\\n",
       "0  требуется разработать агента который оркестрир...   \n",
       "1  обязанности -разрабатывать внедрять оптимизиро...   \n",
       "2  тебе нравятся крутые амбициозные задачи мечтае...   \n",
       "3  тебе нравятся крутые амбициозные задачи мечтае...   \n",
       "4  наша команда каждый день работает повышением к...   \n",
       "\n",
       "                                 requirement_cleaned  \n",
       "0  опыт работы data scientist лет. образование на...  \n",
       "1  уверенное владение python промышленной разрабо...  \n",
       "2  расширение технических знаний развитие аналити...  \n",
       "3  расширение технических знаний развитие аналити...  \n",
       "4  высшее образование сфере экономики математики ...  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea29c2b9-38fe-42ba-939e-616de0c518b4",
   "metadata": {},
   "source": [
    "Чанкование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6d8ac6f5-3ef9-4330-b4d2-f2464452eb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['meta_header'] = (\n",
    "    'Вакансия: ' + df_final['vacancy_name'].fillna('') + '.'\n",
    "    ' Город:  ' + df_final['city_name'].fillna('') + '.' +\n",
    "    \" Опыт: \" + df_final['experience'].fillna('') + \". \" +\n",
    "    \"График: \" + df_final['schedule'].fillna('') + \". \" +\n",
    "    \"Занятость: \" + df_final['employment'].fillna('') + \". \"\n",
    "    \"Минимальная зарплата: \" + df_final['min_salary'].fillna('').astype(str) + \".\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "afb90151-0fc1-4013-93bb-e9a882a8f285",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_requirements = df_final.copy()\n",
    "df_requirements['rag_chunk'] = (df_requirements['meta_header'] +\n",
    "                                'Требования: ' + df_requirements['requirement_cleaned'].fillna('требования не указаны')+ '.'\n",
    "                               \" Обязанности: \" + df_requirements['responsibility'].fillna('Обязанности не указаны.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f3987d2c-5b2e-4b34-8cf6-331f877537d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chunk_coloumn = df_requirements['rag_chunk']\n",
    "df_final = pd.concat([df_final, rag_chunk_coloumn], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b4af2222-94c5-4b6c-8c76-14ae36e44355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Вакансия: Data Scientist (Управление моделирования и исследования данных). Город:  Москва. Опыт: От 3 до 6 лет. График: Полный день. Занятость: Полная занятость. Минимальная зарплата: 0.0.Требования: опыт работы data scientist лет. образование направлениям математика физика мат. методы экономике желательно выпускники мгу мфти.... Обязанности: Сбор требований по моделям и коммуникация с заказчиком и DE. Определение целевых метрик решения и согласование их с заказчиком. '"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_requirements['rag_chunk'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "55d9e903-a05a-4420-9038-1c999300bd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_requirements = df_requirements[['vacancy_id', 'rag_chunk']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "21d924fa-2148-41a2-af57-5743701423a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vacancy_id</th>\n",
       "      <th>rag_chunk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>127489893</td>\n",
       "      <td>Вакансия: Data Scientist (Управление моделиров...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>126119482</td>\n",
       "      <td>Вакансия: Data Scientist Middle/Middle+. Город...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>127117611</td>\n",
       "      <td>Вакансия: Аналитик данных. Город:  Нижний Новг...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>127117612</td>\n",
       "      <td>Вакансия: Аналитик данных. Город:  Самара. Опы...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>126737674</td>\n",
       "      <td>Вакансия: Старший бизнес-аналитик CLTV. Город:...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  vacancy_id                                          rag_chunk\n",
       "0  127489893  Вакансия: Data Scientist (Управление моделиров...\n",
       "1  126119482  Вакансия: Data Scientist Middle/Middle+. Город...\n",
       "2  127117611  Вакансия: Аналитик данных. Город:  Нижний Новг...\n",
       "3  127117612  Вакансия: Аналитик данных. Город:  Самара. Опы...\n",
       "4  126737674  Вакансия: Старший бизнес-аналитик CLTV. Город:..."
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_requirements.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "680ac191-4b02-4f31-855f-3f2935028771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d114e37c-a0d8-40a1-b70b-d985310758fb",
   "metadata": {},
   "source": [
    "Применю различные методы чанкования и позже сравню качество по метрикам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2d0661d0-7040-49c5-b4ad-1ef4d9ebf855",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "01a415e3-d612-46ef-8421-dce7e66f6e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "67a5f963-7035-4d52-8983-2b2d9df61894",
   "metadata": {},
   "outputs": [],
   "source": [
    "#чанковние по приоритету разделителей\n",
    "text_splitter_recursive = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"] # Попытка разбить сначала по абзацам, потом по строкам, потом по пробелам\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e960003-f043-4bd6-a5f4-9d4830b8e7aa",
   "metadata": {},
   "source": [
    "Далее использую продвинутые методы чанкования Fusion Retrieval и HyDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "58f8062c-5e5a-40bb-bece-d94596594933",
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=700, \n",
    "    chunk_overlap=70, \n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"], \n",
    "    length_function=len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2aef62c-0362-4bb8-9dbe-dc3d9749b722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "fc161b53-7da8-4709-a408-c661834c2f87",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SemanticChunker' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[166]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m hyde_splitter = \u001b[43mSemanticChunker\u001b[49m(\n\u001b[32m      2\u001b[39m     embeddings=embeddings,\n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m# Создавать разрывы в точках, где семантическое сходство ниже 75%\u001b[39;00m\n\u001b[32m      4\u001b[39m     breakpoint_threshold_type=\u001b[33m\"\u001b[39m\u001b[33mpercentile\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# threshold=75 (значение по умолчанию)\u001b[39;00m\n\u001b[32m      6\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'SemanticChunker' is not defined"
     ]
    }
   ],
   "source": [
    "'''hyde_splitter = SemanticChunker(\n",
    "    embeddings=embeddings,\n",
    "    # Создавать разрывы в точках, где семантическое сходство ниже 75%\n",
    "    breakpoint_threshold_type=\"percentile\", \n",
    "    # threshold=75 (значение по умолчанию)\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "221c2775-3950-438d-b355-4917d80970bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "f0409ad7-991c-45a4-a6a5-5a1b4d29974a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_general_chunks(row: pd.Series, splitter: Any) -> List[Dict[str, Any]]:\n",
    "   \n",
    "    # 1. Извлечение необходимых данных\n",
    "    meta_header = row['meta_header']\n",
    "    description_text = row['description_cleaned']\n",
    "    # Предполагается, что 'rag_chunk' уже создан и готов к использованию\n",
    "    rag_chunk_text = row['rag_chunk'] \n",
    "    vacancy_id = row['vacancy_id']\n",
    "\n",
    "    general_chunks_list: List[Dict[str, Any]] = []\n",
    "\n",
    "    # --- A. Добавляем короткий, сводный чанк (целиком) ---\n",
    "    # Этот чанк важен для точного поиска по ключевым атрибутам\n",
    "    if pd.notna(rag_chunk_text) and rag_chunk_text.strip():\n",
    "        # Добавляем метку, чтобы LLM понимала, что это \"СВОДКА\"\n",
    "        final_rag_chunk_summary = \"СВОДКА ВАКАНСИИ: \" + rag_chunk_text.strip()\n",
    "        \n",
    "        general_chunks_list.append({\n",
    "            'id': vacancy_id,\n",
    "            'rag_chunk': final_rag_chunk_summary,\n",
    "            'chunk_type': 'summary' \n",
    "        })\n",
    "    \n",
    "    if pd.notna(description_text) and description_text.strip():\n",
    "        \n",
    "        # 2. Разбиение: splitter (будь то Semantic или Recursive) создает объекты Document\n",
    "        chunks_description: List[Document] = splitter.create_documents([description_text])\n",
    "        \n",
    "        # 3. Формирование финальных чанков из описания\n",
    "        for chunk in chunks_description:\n",
    "            chunk_text = chunk.page_content.strip()\n",
    "\n",
    "            # Префикс (метаданные) + метка о типе чанка\n",
    "            final_rag_chunk_detail = (\n",
    "                meta_header + \n",
    "                \" [ДЕТАЛЬНОЕ ОПИСАНИЕ]: \" + chunk_text\n",
    "            )\n",
    "            \n",
    "            general_chunks_list.append({\n",
    "                'id': vacancy_id,\n",
    "                'rag_chunk': final_rag_chunk_detail,\n",
    "                'chunk_type': 'detail'\n",
    "            })\n",
    "            \n",
    "    return general_chunks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "cad8920d-e701-44d6-b333-cb171a2debc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_general_chunks_recursive = []\n",
    "for index, row in df_final.iterrows():\n",
    "    #  Вызываем нашу функцию для получения списка чанков из одной строки\n",
    "    chunks_for_row = create_general_chunks(row, splitter=text_splitter_recursive)\n",
    "    #  Добавляем все чанки из текущей строки в общий список\n",
    "    all_general_chunks_recursive.extend(chunks_for_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "053670a6-4139-4c80-9e92-118110123409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4752"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_general_chunks_recursive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "04e15a81-da93-45a0-8ea1-f24629588c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_general_chunks_recursive = pd.DataFrame(all_general_chunks_recursive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1cdbaf-7968-445d-9808-63dd307ff24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''all_general_chunks_semantic = []\n",
    "for index, row in df_final.iterrows():\n",
    "    #  Вызываем нашу функцию для получения списка чанков из одной строки\n",
    "    chunks_for_row = create_general_chunks(row, splitter=text_splitter_semantic)\n",
    "    #  Добавляем все чанки из текущей строки в общий список\n",
    "    all_general_chunks_semantic.extend(chunks_for_row)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd18d5aa-499c-4947-b416-864b30efb12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''df_general_chunks_semantic = pd.DataFrame(all_general_chunks_semantic)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "efe58d3f-68e0-4134-b526-861a78f1b8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_general_chunks_fusion = []\n",
    "for index, row in df_final.iterrows():\n",
    "    #  Вызываем нашу функцию для получения списка чанков из одной строки\n",
    "    chunks_for_row = create_general_chunks(row, splitter=fusion_splitter)\n",
    "    #  Добавляем все чанки из текущей строки в общий список\n",
    "    all_general_chunks_fusion.extend(chunks_for_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "2f4c0977-902a-4065-a634-465929c21dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_general_chunks_fusion = pd.DataFrame(all_general_chunks_fusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "2f19c002-d4ca-4ed6-be3e-ee46a801cf9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3696"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_general_chunks_fusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eb75cc-7481-4059-992a-7df78c6b79e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''all_general_chunks_hyde = []\n",
    "for index, row in df_final.iterrows():\n",
    "    #  Вызываем нашу функцию для получения списка чанков из одной строки\n",
    "    chunks_for_row = create_general_chunks(row, splitter=hyde_splitter)\n",
    "    #  Добавляем все чанки из текущей строки в общий список\n",
    "    all_general_chunks_hyde.extend(chunks_for_row)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d06aa27-66d9-42b3-9489-abd25823623a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''df_general_chunks_hyde = pd.DataFrame(all_general_chunks_hyde)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c333fd8-7507-4819-a87d-49c6717f22b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''len(all_general_chunks_hyde)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "9c2f32b1-fb4a-4786-879d-4cf0826d0eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'СВОДКА ВАКАНСИИ: Вакансия: Data Scientist в команду антифрода. Город:  Москва. Опыт: От 3 до 6 лет. График: Полный день. Занятость: Полная занятость. Минимальная зарплата: 0.0.Требования: имеете опыт решения различных data science задач использованием python имеете опыт решении практических задач связанных nlp. Обязанности: Улучшать алгоритмы поиска фрода и спама в сервисе, который обрабатывает тысячи событий в минуту. Искать нетривиальные и эффективные решения реальных...'"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_general_chunks_fusion['rag_chunk'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "97f5161d-71a4-4ee4-b3bf-676d78c3d139",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "3f234d85-f81f-490c-afdf-2f9547a7089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "1e567d15-ac2e-4a5b-9e41-1ccc4f564d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_langchain_documents(df):\n",
    "    \"\"\"Преобразует DataFrame чанков в список объектов LangChain Document.\"\"\"\n",
    "    documents = []\n",
    "    for index, row in df.iterrows():\n",
    "        # Важно: Сохраняем ID, чтобы можно было отследить источник\n",
    "        # и сохраняем тип чанка, чтобы различать сводку и детали.\n",
    "        documents.append(\n",
    "            Document(\n",
    "                page_content=row['rag_chunk'],\n",
    "                metadata={\n",
    "                    \"id\": row['id']\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "cc50b581-9339-4b9c-a9b1-d6058b7af2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- A. База для РЕКУРСИВНОГО чанкования (Base RAG) ---\n",
    "recursive_docs = df_to_langchain_documents(df_general_chunks_recursive)\n",
    "db_recursive = FAISS.from_documents(recursive_docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87759b6-6f09-4d82-98af-50ae302d63a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- B. База для СЕМАНТИЧЕСКОГО чанкования (для сравнения) ---\n",
    "'''semantic_docs = df_to_langchain_documents(df_general_chunks_semantic)\n",
    "db_semantic = FAISS.from_documents(semantic_docs, embeddings)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2704aac9-2389-469b-814d-980ac32b625b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- C. База для HYDE ---\n",
    "# HyDE использует свой набор чанков.\n",
    "'''hyde_docs = df_to_langchain_documents(df_general_chunks_hyde)\n",
    "db_hyde = FAISS.from_documents(hyde_docs, embeddings)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "88c2a168-29d3-4e31-b843-8174cf843d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- D. База для FUSION RETRIEVAL (BM25 + FAISS) ---\n",
    "# Fusion требует два ретривера, но оба используют ОДИН И ТОТ ЖЕ набор чанков.\n",
    "fusion_docs = df_to_langchain_documents(df_general_chunks_fusion)\n",
    "db_fusion_dense = FAISS.from_documents(fusion_docs, embeddings) # Dense Retriever (Faiss)\n",
    "# BM25 Retriever (Sparse) будет создан из fusion_docs, используя специальный класс LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "4b633511-2456-4aed-a241-c8fde335ffa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet\n",
    "df_general_chunks_fusion.to_parquet('df_fusion_chunks.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "17d095e1-2f84-441f-b006-ef4a02171afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "База данных Recursive сохранена в: ./faiss_index_recursive2\n"
     ]
    }
   ],
   "source": [
    "# 1. База для РЕКУРСИВНОГО чанкования\n",
    "index_path_recursive = \"./faiss_index_recursive2\"\n",
    "db_recursive.save_local(index_path_recursive)\n",
    "print(f\"База данных Recursive сохранена в: {index_path_recursive}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15235f02-46b3-4621-844f-204c04ad1689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. База для СЕМАНТИЧЕСКОГО чанкования\n",
    "'''index_path_semantic = \"./faiss_index_semantic\"\n",
    "db_semantic.save_local(index_path_semantic)\n",
    "print(f\"База данных Semantic сохранена в: {index_path_semantic}\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c55234-7ac0-456e-b9a0-f72243b1be5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. База для HYDE\n",
    "'''index_path_hyde = \"./faiss_index_hyde\"\n",
    "db_hyde.save_local(index_path_hyde)\n",
    "print(f\"База данных HyDE сохранена в: {index_path_hyde}\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "d2c2ae6f-5109-43da-b493-f801c66100d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "База данных Fusion Dense сохранена в: ./faiss_index_fusion_2\n"
     ]
    }
   ],
   "source": [
    "# 4. База для FUSION RETRIEVAL (Dense Index)\n",
    "index_path_fusion = \"./faiss_index_fusion_2\"\n",
    "db_fusion_dense.save_local(index_path_fusion)\n",
    "print(f\"База данных Fusion Dense сохранена в: {index_path_fusion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be6e1fa-1f3e-406b-9a0a-6058a45f6f95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f8ca95-8cc4-42f9-a905-80431dafcce6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG Project (rag_final)",
   "language": "python",
   "name": "rag_final"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
